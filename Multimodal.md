# Multimodal pretrain
- Two mothods:1.Bert. seq2seq model. 2.CLIP. constrastive pretrained model.

# Muiltimodal Paper
- [Tutorial on Multimodal Machine Learning](https://www.cs.cmu.edu/~morency/MMML-Tutorial-ACL2017.pdf)
- [Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text](https://arxiv.org/pdf/1809.00782.pdf) [blog](/Mutilmodal/Open%20Domain%20Question%20Answering%20Using%20Early%20Fusion%20of%20Knowledge%20Bases%20and%20Text.md)


待看论文：
language model：
1. metaLM(Microsoft)
2. Poli(Google)
generalist model
1. Unified IO
2. Uniperciver (V2)
3. Uniperciver MOE

解决的问题：
1. 计算效率问题


论文关系：
![paper relation](多模态.png)
